{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning with Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaifullahKatpar/PySpark/blob/master/Machine_Learning_with_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-InzgEM5qb6",
        "colab_type": "text"
      },
      "source": [
        "# Setup Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eot0RCw_BpDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD44d82fB4tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYsMbVgyCCWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qTstjne51kn",
        "colab_type": "text"
      },
      "source": [
        "# Setup Drive for Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT4_qRgN54_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQpZah9n6DJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Colab Notebooks/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEp69y45hEE",
        "colab_type": "text"
      },
      "source": [
        "# Creating a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_y7IShcCFcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fe75263-4dd2-4880-fff3-3b33d277fee0"
      },
      "source": [
        "# Import the PySpark module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession object\n",
        "spark = SparkSession.builder \\\n",
        "                    .master('local[*]') \\\n",
        "                    .appName('test') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "\n",
        "# What version of Spark?\n",
        "print(spark.version)\n",
        "\n",
        "# Terminate the cluster\n",
        "# spark.stop()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc9sgiCm5ag6",
        "colab_type": "text"
      },
      "source": [
        "# Loading flights data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbgrFzLCUaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "58d9efb6-bbb1-4daa-ea6d-1769b1466bb7"
      },
      "source": [
        "# Read data from CSV file\n",
        "flights = spark.read.csv(path+'flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())\n",
        "\n",
        "# View the first five records\n",
        "flights.show(5)\n",
        "\n",
        "# Check column data types\n",
        "flights.dtypes"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data contain 50000 records.\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mon', 'int'),\n",
              " ('dom', 'int'),\n",
              " ('dow', 'int'),\n",
              " ('carrier', 'string'),\n",
              " ('flight', 'int'),\n",
              " ('org', 'string'),\n",
              " ('mile', 'int'),\n",
              " ('depart', 'double'),\n",
              " ('duration', 'int'),\n",
              " ('delay', 'int')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqq-dSMp5VaE",
        "colab_type": "text"
      },
      "source": [
        "# Loading SMS spam data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeFdabryCZ51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3624d81f-fae2-4d4d-a739-962e77663f70"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Specify column names and types\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"text\", StringType()),\n",
        "    StructField(\"label\", IntegerType())\n",
        "])\n",
        "\n",
        "# Load data from a delimited file\n",
        "sms = spark.read.csv(path+'sms.csv', sep=';', header=False, schema=schema)\n",
        "\n",
        "# Print schema of DataFrame\n",
        "sms.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBHsvpjs5SFf",
        "colab_type": "text"
      },
      "source": [
        "# Removing columns and rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egHH7xKMFyCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce57f4f9-a41c-410b-830f-b64ae4f9532f"
      },
      "source": [
        "# Remove the 'flight' column\n",
        "flights = flights.drop('flight')\n",
        "\n",
        "# Number of records with missing 'delay' values\n",
        "flights.filter('delay IS NULL').count()\n",
        "\n",
        "# Remove records with missing 'delay' values\n",
        "flights = flights.filter('delay IS NOT NULL')\n",
        "\n",
        "# Remove records with missing values in any column and get the number of remaining rows\n",
        "flights = flights.dropna()\n",
        "print(flights.count())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO0Pd5ev5N_N",
        "colab_type": "text"
      },
      "source": [
        "# Column manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yofq_4F_VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "c5f75677-d3de-43b4-ad7f-2bfc97f1482a"
      },
      "source": [
        "\n",
        "# Import the required function\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
        "                    .drop('mile')\n",
        "\n",
        "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay>=15).cast('integer'))\n",
        "\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
            "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
            "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
            "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
            "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_aG_8N5Lda",
        "colab_type": "text"
      },
      "source": [
        "# Categorical columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn4TETnrGIql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
        "\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights_km)\n",
        "\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights_km)\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTddJ7hJ5DbJ",
        "colab_type": "text"
      },
      "source": [
        "# Assembling columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JO80fYNGQ9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a5cb5b3f-1706-440b-bd51-90002166811d"
      },
      "source": [
        "# Import the necessary class\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=[\n",
        "  'mon','dom','dow','carrier_idx','org_idx','km',\n",
        "  'depart','duration'                                     \n",
        "], outputCol='features')\n",
        "\n",
        "# Consolidated predictor columns\n",
        "flights_assembled = assembler.transform(flights_indexed)\n",
        "\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features','delay').show(5,truncate=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+-----+\n",
            "|features                                 |delay|\n",
            "+-----------------------------------------+-----+\n",
            "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
            "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
            "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
            "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
            "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
            "+-----------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbx-dWSx49bT",
        "colab_type": "text"
      },
      "source": [
        "# Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqHFYndvJyfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "327d2432-a535-46d6-8b06-e2f7bc2b8746"
      },
      "source": [
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2],seed=17)\n",
        "\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights_assembled.count()\n",
        "print(training_ratio)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7980732423121092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXPwuzJo44so",
        "colab_type": "text"
      },
      "source": [
        "# Build a Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVIroPC_LoNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "22602908-7e7b-462e-a56d-179d620aad48"
      },
      "source": [
        "\n",
        "# Import the Decision Tree Classifier class\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Create a classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+----------------------------------------+\n",
            "|label|prediction|probability                             |\n",
            "+-----+----------+----------------------------------------+\n",
            "|1    |1.0       |[0.48797920727745286,0.5120207927225471]|\n",
            "|1    |1.0       |[0.4045788484537844,0.5954211515462157] |\n",
            "|1    |1.0       |[0.3260848648128548,0.6739151351871452] |\n",
            "|1    |1.0       |[0.3260848648128548,0.6739151351871452] |\n",
            "|1    |1.0       |[0.3260848648128548,0.6739151351871452] |\n",
            "+-----+----------+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cp_T4ZR42Nd",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkK7upLtMtkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "1e18604b-e2fc-47ed-ce15-adf0b06feb42"
      },
      "source": [
        "\n",
        "# Create a confusion matrix\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN+TP)/(TN+TP+FN+FP)\n",
        "print(accuracy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1084|\n",
            "|    0|       0.0| 2286|\n",
            "|    1|       1.0| 3740|\n",
            "|    0|       1.0| 2385|\n",
            "+-----+----------+-----+\n",
            "\n",
            "0.6346498156924697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2dIbdl34rXy",
        "colab_type": "text"
      },
      "source": [
        "# Build a Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2BWA99k4syU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e440919c-699f-4c9f-ea5e-d76ba455292b"
      },
      "source": [
        "# Import the logistic regression class\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a classifier object and train on training data\n",
        "logistic = LogisticRegression().fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and show confusion matrix\n",
        "prediction = logistic.transform(flights_test)\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1652|\n",
            "|    0|       0.0| 2645|\n",
            "|    1|       1.0| 3172|\n",
            "|    0|       1.0| 2026|\n",
            "+-----+----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx__2Jim4vxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}